{"version":3,"sources":["webpack://shoaib-alyaan-portfolio/./src/components/Gohome.js","webpack://shoaib-alyaan-portfolio/./src/style/gohome.module.css","webpack://shoaib-alyaan-portfolio/./src/style/nav.module.css","webpack://shoaib-alyaan-portfolio/./src/components/Nav.js","webpack://shoaib-alyaan-portfolio/./src/components/Page.js","webpack://shoaib-alyaan-portfolio/./src/components/animation.js","webpack://shoaib-alyaan-portfolio/./src/style/agri.module.css","webpack://shoaib-alyaan-portfolio/./src/pages/projects/agri.js"],"names":["className","Nav","Link","to","viewBox","fill","xmlns","d","stroke","strokeWidth","bar","item","useState","open","setOpen","useEffect","Object","assign","document","body","style","height","overflow","setTimeout","element","getElementById","display","chain","elements","transform","opacity","delay","onClick","state","styles","id","aria-current","href","children","title","props","parent","map","index","apply","querySelector","scrollAnimate","cname","cArray","Array","prototype","slice","call","childNodes","view","child","styleClass","classList","add","querySelectorAll","forEach","rstyle","remove","top","bottom","bounding","getBoundingClientRect","window","innerHeight","figName","data","filteredData","agri","edges","edge","node","name","childImageSharp","gatsbyImageData","Page","margin","textAlign","target","rel","color","textDecoration","G","image","block","listStyleType","inception","transfer","proposed","maxWidth","verticalAlign","fontSize","snippet","landing","Report","table","matrix"],"mappings":"6MA0CA,MArCA,WACE,OACE,uBAAKA,UCNgB,mCDOnB,uBAAKA,UCNW,gCDOd,gBAACC,EAAA,EAAD,MACA,sBAAID,UCNQ,8BDOV,gBAAC,EAAAE,KAAD,CAAMC,GAAG,KACP,uBACEH,UCVK,4BDWLI,QAAQ,cACRC,KAAK,OACLC,MAAM,8BAEN,wBACEC,EAAE,4JACFF,KAAK,UACLG,OAAO,QACPC,YAAY,MAEd,wBAAMF,EAAE,kCAAkCF,KAAK,SAC/C,wBACEE,EAAE,kJACFC,OAAO,QACPC,YAAY,MAEd,wBACEF,EAAE,0wFACFF,KAAK,iB,wFE/Bd,MAGMK,EAAM,yBASNC,EAAO,0BCsEpB,MA/EA,WAAgB,IAAD,GACWC,eAAS,GAA1BC,EADM,KACAC,EADA,KAyCb,OAjCAC,gBAAU,WA8BRF,GAnCAG,OAAOC,OAAOC,SAASC,KAAKC,MAAO,CAAEC,OAAO,QAAUC,SAAS,WAuB7DC,YAAW,WACT,IAAIC,EAAUN,SAASO,eAAe,gBACtCT,OAAOC,OAAOO,EAAQJ,MAAO,CAAEM,QAAS,UAExCC,QAAM,CACJC,SAAU,CAAC,QAAS,cAAe,aACnCR,MAAO,CAAES,UAAW,iBAAkBC,QAAS,KAC/CC,MAAO,QAER,OA7BLf,OAAOC,OAAOC,SAASC,KAAKC,MAAO,CAAEC,OAAO,OAASC,SAAS,YAM5DK,QAAM,CACJC,SAAU,CAAC,QAAS,cAAe,aACnCR,MAAO,CAAES,UAAW,kBAAmBC,QAAS,KAChDC,MAAO,MAGTR,YAAW,WACT,IAAIC,EAAUN,SAASO,eAAe,gBACtCT,OAAOC,OAAOO,EAAQJ,MAAO,CAAEM,QAAS,WACvC,QAkBJ,CAACb,IAGF,uBAAKb,UD7CgB,gCC8CnB,uBACEgC,QAAS,WACPlB,GAAQ,SAAAmB,GAAK,OAAKA,MAEpBjC,UAAWa,EDhDM,+BADD,+BCmDhB,uBAAKb,UAAckC,+BACnB,uBAAKlC,UAAckC,+BACnB,uBAAKlC,UAAckC,gCAGrB,uBAAKC,GAAG,eAAenC,UDjDA,oCCkDrB,sBAAIA,UAAWa,EDhDK,kCADE,qCCkDpB,sBAAIb,UDhDU,+BCgDd,iBACA,sBAAImC,GAAG,OAAOnC,UAAWkC,GACvB,qBAAGE,eAAa,OAAOpC,UAAU,GAAGqC,KAAK,KACvC,oCAGJ,sBAAIF,GAAG,aAAanC,UAAWkC,GAC7B,qBAAGG,KAAK,gBACN,2CAGJ,sBAAIF,GAAG,WAAWnC,UAAWkC,GAC3B,qBAAGG,KAAK,aACN,mDAKR,uBAAKrC,UAAWa,EDhEA,4BACC,kC,mEEDvB,IAVA,YAAmC,IAAnByB,EAAkB,EAAlBA,SAASC,EAAS,EAATA,MACvB,OACE,2BACE,gBAAC,IAAD,CAAKA,MAAOA,IACZ,gBAAC,IAAD,MACCD,K,gCCTA,SAASX,EAAMa,GAChBA,GAASA,EAAMC,QAERD,GAASA,EAAMZ,WAEE,iBAAfY,EAAMT,MACfS,EAAMZ,SAASc,KAAI,SAAClB,EAASmB,GAE3BpB,YAAW,WACTqB,EAAMpB,EAASgB,EAAMpB,SACpBoB,EAAMT,MAAQY,EAAQH,EAAMT,UAMjCS,EAAMZ,SAASc,KAAI,SAAClB,EAASmB,GAC3BpB,YAAW,WACTqB,EAAMpB,EAASgB,EAAMpB,SACpBoB,EAAMT,MAAMY,GAASA,EAAQH,EAAMT,MAAMY,Q,yDAMpD,IAAMC,EAAQ,SAACpB,EAASJ,GACtBJ,OAAOC,OAAOC,SAAS2B,cAAcrB,GAASJ,MAAOA,IAO1C0B,EAAgB,SAAAN,GAE3B,GAAIA,EAAMO,MAER,GAAsB,GAAlBP,EAAMF,SAAkB,CAC1B,IAAIU,EAEAP,EAASvB,SAAS2B,cAAT,IAA2BL,EAAMO,OAE9CC,EAASC,MAAMC,UAAUC,MAAMC,KAC7BlC,SAAS2B,cAAT,IAA2BL,EAAMO,OAASM,YAGxCb,EAAMpB,MACJkC,EAAKb,IACPO,EAAON,KAAI,SAACa,EAAOZ,GACjBpB,YAAW,WACTP,OAAOC,OAAOsC,EAAMnC,MAAOoB,EAAMpB,SAChCoB,EAAMT,MAAQY,EAAQH,EAAMT,UAI1BS,EAAMgB,YAEXF,EAAKb,IACPO,EAAON,KAAI,SAACa,EAAOZ,GACjBpB,YAAW,WACTgC,EAAME,UAAUC,IAAIlB,EAAMgB,cACzBhB,EAAMT,MAAQY,EAAQH,EAAMT,eAOrCiB,EAASC,MAAMC,UAAUC,MAAMC,KAC7BlC,SAASyC,iBAAT,IAA8BnB,EAAMO,QAGlCP,EAAMpB,MACR4B,EAAOY,SAAQ,SAAApC,GACT8B,EAAK9B,GACPR,OAAOC,OAAOO,EAAQJ,MAAOoB,EAAMpB,OAEnCJ,OAAOC,OAAOO,EAAQJ,MAAOoB,EAAMqB,WAI9BrB,EAAMgB,YACfR,EAAOY,SAAQ,SAAApC,GACT8B,EAAK9B,GACPA,EAAQiC,UAAUC,IAAIlB,EAAMgB,YAE5BhC,EAAQiC,UAAUK,OAAOtB,EAAMgB,gBAS9BF,EAAO,SAAC9B,EAASuC,EAAWC,QAAiB,IAA5BD,MAAM,SAAsB,IAAjBC,MAAS,IAChD,IAAIC,EAAWzC,EAAQ0C,wBACvB,OACED,EAASF,IAAMI,OAAOC,YAAcL,GACpCE,EAASF,IAAMI,OAAOC,YAAcJ,I,sGClGjC,MACM,EAAQ,4BACRK,EAAU,8B,aCmXhB,IAcP,EA9XA,YAAyB,IAATC,EAAQ,EAARA,KACVC,EAAe,GAMnB,OAJAD,EAAKE,KAAKC,MAAMb,SAAQ,SAAAc,GACtBH,EAAaG,EAAKC,KAAKC,MAAQF,EAAKC,KAAKE,gBAAgBC,mBAIzD,gBAACC,EAAA,EAAD,CAAMxC,MAAM,qBACV,uBAAKvC,UDdc,iCCejB,uCACA,sBAAIoB,MAAO,CAAE4D,OAAQ,eAAgBC,UAAW,WAAhD,kBACkB,IAChB,qBACEC,OAAO,SACPC,IAAI,sBACJ9C,KAAK,oCACLjB,MAAO,CAAEgE,MAAO,OAAQC,eAAgB,cAJ1C,WASF,uBAAKrF,UAAWkC,GACd,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAaiB,QACjC,qBAAGxF,UAAWkC,GAAd,6BAEF,2BACE,2BACE,0CACA,wUAOA,mCACA,sCACa,IACX,qBACEG,KAAK,qDACL6C,OAAO,SACPC,IAAI,sBACJ/D,MAAO,CAAEgE,MAAO,SAJlB,gBAOK,IATP,gaAkBA,2CACA,sBAAIhE,MAAO,CAAEqE,cAAe,SAC1B,mHAIA,oGAQR,0CACA,iHAIA,0BACE,6GAIA,oJAIA,+LAMF,0CACA,2WAOE,2BACA,2BARF,mFAYA,sBAAIrE,MAAO,CAAEqE,cAAe,SAC1B,2GAIA,mJAKF,2BAxGF,4UA6GkD,IAChD,2BACE,gBAAC,EAAAH,EAAD,CAAaC,MAAOhB,EAAamB,YACjC,qBAAG1F,UAAWkC,GAAd,4BAEF,2HAIA,0BACE,0BACE,yBACE,+CADF,0PAKmB,KAEnB,0BACE,mEACA,4DACA,qHAMJ,0BACE,yBACE,8CADF,2WASF,0BACE,yBACE,2CADF,qlBAaF,0BACE,yBACE,mDADF,kMAOF,0BACE,yBACE,2CADF,2vBAiBJ,41BAeA,+CACA,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAaoB,WACjC,qBAAG3F,UAAWkC,GAAd,2BAEF,+KAIE,2BAJF,kJAQE,2BARF,4UAeA,qIAGE,2BAHF,gFAKE,2BALF,mNASE,2BATF,0NAaE,2BACA,2BAdF,8GAe8C,2BAC5C,2BAhBF,oTAuBA,4CACA,2BACE,gBAAC,EAAAoD,EAAD,CACEC,MAAOhB,EAAaqB,SACpBxE,MAAO,CAAEyE,SAAU,QAASb,OAAQ,UACtC,qBAAGhF,UAAWkC,GAAd,wBAEF,yBACE,oCADF,sDAEE,2BACA,oCAHF,+BAIE,2BACA,oCALF,sCAME,2BACA,oCAPF,qIASe,2BACb,oCAVF,mBAUgC,2BAC9B,oCAXF,mQAeE,2BACA,2BACA,gCAEE,wBAAMd,MAAO,CAAE0E,cAAe,QAASC,SAAU,SAAjD,MAAqE,IAFvE,UAIK,IArBP,mEAsBkE,2BAChE,gCAEE,wBAAM3E,MAAO,CAAE0E,cAAe,QAASC,SAAU,SAAjD,MAAqE,IAFvE,UAIK,IA3BP,oHA8BE,2BACA,2BACA,qBAAG3E,MAAO,CAAE2E,SAAU,SAAtB,YAhCF,aAgCyD,IACvD,8CAjCF,mNAoCuB,mDApCvB,kOAwCK,IACH,kJAzCF,gEA6CgE,IAC9D,yCA9CF,0BAgDA,2BACE,gBAAC,EAAAT,EAAD,CACEC,MAAOhB,EAAayB,QACpB5E,MAAO,CAAEyE,SAAU,QAASb,OAAQ,UACtC,qBAAGhF,UAAWkC,GAAd,4BAEF,qCACA,+LAIE,2BAJF,iIAQA,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAa0B,UACjC,qBAAGjG,UAAWkC,GAAd,2CAIF,qNAKA,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAa2B,OAAQ9E,MAAO,CAAEyE,SAAU,QAASb,OAAQ,UAC7E,qBAAGhF,UAAWkC,GAAd,qCAEF,yBACG,IADH,4aAUA,uBAAKlC,UAAWkC,GACd,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAa4B,QACjC,qBAAGnG,UAAWkC,GAAd,kEAKF,2BACE,gBAAC,EAAAoD,EAAD,CAAaC,MAAOhB,EAAa6B,SACjC,qBAAGpG,UAAWkC,GAAd","file":"component---src-pages-projects-agri-js-cd7ebe29d47db9d04efa.js","sourcesContent":["import React from \"react\"\nimport * as styles from \"../style/gohome.module.css\"\nimport Nav from \"./Nav\"\nimport { Link } from \"gatsby\"\n\nfunction Gohome() {\n  return (\n    <div className={styles.container}>\n      <div className={styles.NavBar}>\n        <Nav />\n        <h2 className={styles.home}>\n          <Link to=\"/\">\n            <svg\n              className={styles.SHO}\n              viewBox=\"0 0 258 207\"\n              fill=\"none\"\n              xmlns=\"http://www.w3.org/2000/svg\"\n            >\n              <path\n                d=\"M246 154H155.5H12C7.02944 154 3 149.971 3 145V11.5C3 6.52944 7.02943 2.5 12 2.5H246C250.971 2.5 255 6.52943 255 11.5V145C255 149.971 250.971 154 246 154Z\"\n                fill=\"#E94F37\"\n                stroke=\"black\"\n                strokeWidth=\"5\"\n              ></path>\n              <path d=\"M104 155.5V204H154.5V155.5H104Z\" fill=\"none\"></path>\n              <path\n                d=\"M68.5001 204H187M113.5 122.5L128.5 81L143.5 39.5M91.5001 110L31.0001 78.5L91.5001 47M168.5 110L224.5 78.5L168.5 47M104 204V155.5H154.5V204H104Z\"\n                stroke=\"black\"\n                strokeWidth=\"5\"\n              ></path>\n              <path\n                d=\"M99.205 106.65C96.475 106.65 94.3517 106.26 92.835 105.48C91.3617 104.7 90.625 103.812 90.625 102.815C90.625 102.252 90.7333 101.623 90.95 100.93C91.21 100.237 91.5567 99.6083 91.99 99.045C93.68 100.562 95.9117 101.32 98.685 101.32C102.975 101.32 105.12 99.11 105.12 94.69C105.12 93.13 104.838 91.7433 104.275 90.53C103.712 89.2733 102.715 87.8 101.285 86.11L94.785 78.505C93.3117 76.7283 92.25 75.125 91.6 73.695C90.9933 72.2217 90.69 70.705 90.69 69.145C90.69 66.3283 91.665 64.075 93.615 62.385C95.6083 60.695 98.36 59.85 101.87 59.85C104.773 59.85 106.918 60.1533 108.305 60.76C109.692 61.3233 110.385 62.1033 110.385 63.1C110.385 64.1833 109.952 65.18 109.085 66.09C108.608 65.7 107.763 65.3533 106.55 65.05C105.38 64.7033 104.102 64.53 102.715 64.53C100.765 64.53 99.205 64.92 98.035 65.7C96.9083 66.48 96.345 67.585 96.345 69.015C96.345 70.1417 96.5833 71.1817 97.06 72.135C97.5367 73.045 98.295 74.15 99.335 75.45L106.42 83.445C108.067 85.395 109.193 87.1933 109.8 88.84C110.45 90.4867 110.775 92.35 110.775 94.43C110.775 98.1133 109.865 101.082 108.045 103.335C106.268 105.545 103.322 106.65 99.205 106.65ZM136.021 104.96C136.021 105.35 135.934 105.61 135.761 105.74C135.631 105.87 135.133 105.957 134.266 106C133.443 106 132.294 106 130.821 106V83.965H120.421V104.96C120.421 105.35 120.334 105.61 120.161 105.74C120.031 105.87 119.533 105.957 118.666 106C117.799 106 116.629 106 115.156 106V61.54C115.156 61.15 115.221 60.89 115.351 60.76C115.524 60.63 116.044 60.565 116.911 60.565C117.778 60.5217 118.948 60.5 120.421 60.5V79.025H130.821V61.54C130.821 61.15 130.886 60.89 131.016 60.76C131.189 60.63 131.688 60.565 132.511 60.565C133.378 60.5217 134.548 60.5 136.021 60.5V104.96ZM152.671 106.65C149.724 106.65 147.406 105.978 145.716 104.635C144.069 103.248 142.856 100.865 142.076 97.485C141.296 94.0617 140.906 89.23 140.906 82.99C140.906 77.27 141.339 72.72 142.206 69.34C143.116 65.96 144.481 63.5333 146.301 62.06C148.121 60.5867 150.483 59.85 153.386 59.85C156.246 59.85 158.499 60.565 160.146 61.995C161.793 63.3817 162.984 65.7217 163.721 69.015C164.501 72.3083 164.891 76.9017 164.891 82.795C164.891 88.7317 164.458 93.4333 163.591 96.9C162.768 100.367 161.468 102.858 159.691 104.375C157.914 105.892 155.574 106.65 152.671 106.65ZM152.866 101.255C154.469 101.255 155.726 100.648 156.636 99.435C157.546 98.2217 158.196 96.2933 158.586 93.65C159.019 91.0067 159.236 87.4317 159.236 82.925C159.236 78.4183 159.063 74.9083 158.716 72.395C158.369 69.8817 157.763 68.0617 156.896 66.935C156.029 65.8083 154.816 65.245 153.256 65.245C151.696 65.245 150.439 65.8733 149.486 67.13C148.576 68.3433 147.904 70.3367 147.471 73.11C147.038 75.84 146.821 79.5667 146.821 84.29C146.821 88.45 146.994 91.765 147.341 94.235C147.731 96.6617 148.359 98.4383 149.226 99.565C150.093 100.692 151.306 101.255 152.866 101.255Z\"\n                fill=\"white\"\n              ></path>\n            </svg>\n          </Link>\n        </h2>\n      </div>\n    </div>\n  )\n}\n\nexport default Gohome\n","// extracted by mini-css-extract-plugin\nexport const container = \"gohome-module--container--s_Y8P\";\nexport const NavBar = \"gohome-module--NavBar--20YcT\";\nexport const SHO = \"gohome-module--SHO--2h-T0\";\nexport const home = \"gohome-module--home--3y76k\";","// extracted by mini-css-extract-plugin\nexport const container = \"nav-module--container--1uXNs\";\nexport const openIcon = \"nav-module--openIcon--kco8V\";\nexport const closeIcon = \"nav-module--closeIcon--3m9w7\";\nexport const bar = \"nav-module--bar--1Fbv1\";\nexport const bar1 = \"nav-module--bar1--2cQv9\";\nexport const bar2 = \"nav-module--bar2--3G4C7\";\nexport const bar3 = \"nav-module--bar3--1_HUK\";\nexport const side2side = \"nav-module--side2side--3TAiH\";\nexport const itemContainer = \"nav-module--itemContainer--8kuFG\";\nexport const itemsInvisible = \"nav-module--itemsInvisible--1satG\";\nexport const itemsVisible = \"nav-module--itemsVisible--3EL9m\";\nexport const navTitle = \"nav-module--navTitle--dG0n4\";\nexport const item = \"nav-module--item--2VeQd\";\nexport const openBG = \"nav-module--openBG--1_EfO\";\nexport const closeBG = \"nav-module--closeBG--2r85t\";","import React, { useState, useEffect } from \"react\"\nimport { chain } from \"./animation\"\nimport * as styles from \"../style/nav.module.css\"\n\nfunction Nav() {\n  const [open, setOpen] = useState(false)\n  const preventScroll = () => {\n    Object.assign(document.body.style, { height: `100vh`, overflow: `hidden` })\n  }\n  const enableScroll = () => {\n    Object.assign(document.body.style, { height: `100%`, overflow: `scroll` })\n  }\n  useEffect(() => {\n    //   Chain Animations on close\n    const closeAni = () => {\n      enableScroll()\n      chain({\n        elements: [\"#Home\", \"#Internship\", \"#Aproject\"],\n        style: { transform: \"translate(50px)\", opacity: \"0\" },\n        delay: 100,\n      })\n      // After closing make the options disappear\n      setTimeout(() => {\n        var element = document.getElementById(\"NavContainer\")\n        Object.assign(element.style, { display: \"none\" })\n      }, 1000)\n    }\n    //   Chain Animations on open(Additional Timeout to wait for title to appear)\n    const openAni = () => {\n      preventScroll()\n      setTimeout(() => {\n        var element = document.getElementById(\"NavContainer\")\n        Object.assign(element.style, { display: \"flex\" })\n        // ^ Make the section display flex from none to make it visible\n        chain({\n          elements: [\"#Home\", \"#Internship\", \"#Aproject\"],\n          style: { transform: \"translate(0px)\", opacity: \"1\" },\n          delay: 100,\n        })\n      }, 1000)\n    }\n    // Condition to check if state is changed\n    open ? openAni() : closeAni()\n  }, [open])\n\n  return (\n    <div className={styles.container}>\n      <div\n        onClick={() => {\n          setOpen(state => !state)\n        }}\n        className={open ? styles.closeIcon : styles.openIcon}\n      >\n        <div className={`${styles.bar} ${styles.bar1}`}></div>\n        <div className={`${styles.bar} ${styles.bar2}`}></div>\n        <div className={`${styles.bar} ${styles.bar3}`}></div>\n      </div>\n\n      <nav id=\"NavContainer\" className={styles.itemContainer}>\n        <ul className={open ? styles.itemsVisible : styles.itemsInvisible}>\n          <li className={styles.navTitle}>SHOAIB ALYAAN</li>\n          <li id=\"Home\" className={styles.item}>\n            <a aria-current=\"page\" className=\"\" href=\"/\">\n              <h2>Home</h2>\n            </a>\n          </li>\n          <li id=\"Internship\" className={styles.item}>\n            <a href=\"/internships\">\n              <h2>Internships</h2>\n            </a>\n          </li>\n          <li id=\"Aproject\" className={styles.item}>\n            <a href=\"/projects\">\n              <h2>Academic Projects</h2>\n            </a>\n          </li>\n        </ul>\n      </nav>\n      <div className={open ? styles.openBG : styles.closeBG}></div>\n    </div>\n  )\n}\n\nexport default Nav\n","import React from \"react\";\nimport SEO from \"./seo\";\nimport Gohome from \"./Gohome\";\n\nfunction Page({ children,title }) {\n  return (\n    <div>\n      <SEO title={title} />\n      <Gohome />\n      {children}\n    </div>\n  );\n}\n\nexport default Page;\n","export function chain(props) {\n  if (props && props.parent) {\n    // console.log(\"Parent\", props.parent)\n  } else if (props && props.elements) {\n    // only one delay\n    if (typeof props.delay == typeof 5) {\n      props.elements.map((element, index) => {\n        // console.log(typeof props.delay, \"TYOE\")\n        setTimeout(() => {\n          apply(element, props.style)\n        }, props.delay + index * props.delay)\n      })\n    }\n    // More than one delay\n    else {\n      // console.log(\"ARRAYYYYY\")\n      props.elements.map((element, index) => {\n        setTimeout(() => {\n          apply(element, props.style)\n        }, props.delay[index] + index * props.delay[index])\n      })\n    }\n  }\n}\n\nconst apply = (element, style) => {\n  Object.assign(document.querySelector(element).style, style)\n}\n\n// cname: Class which is to be selected\n// styleClass: Class which is to be applied\n// delay: the delay between application of styling to children\n// children: boolean to tell if children are to be selected or parent\nexport const scrollAnimate = props => {\n  // If cname is given then select that class\n  if (props.cname) {\n    // If children of the class provided need to be applied with the styleClass(when parent is visible)\n    if (props.children == true) {\n      var cArray\n      // Get the parent element\n      var parent = document.querySelector(`.${props.cname}`)\n      // Get an array of all children of parent\n      cArray = Array.prototype.slice.call(\n        document.querySelector(`.${props.cname}`).childNodes\n      )\n      // If a style is given to apply to the children\n      if (props.style) {\n        if (view(parent)) {\n          cArray.map((child, index) => {\n            setTimeout(() => {\n              Object.assign(child.style, props.style)\n            }, props.delay + index * props.delay)\n          })\n        }\n        // If class is given to apply to the children\n      } else if (props.styleClass) {\n        // If parent is in view then apply the style to the children at the specified delay\n        if (view(parent)) {\n          cArray.map((child, index) => {\n            setTimeout(() => {\n              child.classList.add(props.styleClass)\n            }, props.delay + index * props.delay)\n          })\n        }\n      }\n    }\n    // If the elements with cname are to be selected(and not children)\n    else {\n      cArray = Array.prototype.slice.call(\n        document.querySelectorAll(`.${props.cname}`)\n      )\n      // If style is given to apply\n      if (props.style) {\n        cArray.forEach(element => {\n          if (view(element)) {\n            Object.assign(element.style, props.style)\n          } else {\n            Object.assign(element.style, props.rstyle)\n          }\n        })\n        // If class is given to apply\n      } else if (props.styleClass) {\n        cArray.forEach(element => {\n          if (view(element)) {\n            element.classList.add(props.styleClass)\n          } else {\n            element.classList.remove(props.styleClass)\n          }\n        })\n      }\n    }\n  }\n}\n\n// Check if an element is within the viewport as specified by top and bottom\nexport const view = (element, top = 0.2, bottom = 0.8) => {\n  var bounding = element.getBoundingClientRect()\n  if (\n    bounding.top > window.innerHeight * top &&\n    bounding.top < window.innerHeight * bottom\n  ) {\n    return true\n  } else {\n    return false\n  }\n}\n","// extracted by mini-css-extract-plugin\nexport const container = \"agri-module--container--3Nrft\";\nexport const float = \"agri-module--float--16NsZ\";\nexport const figName = \"agri-module--figName---liQI\";","import React from \"react\"\nimport Page from \"../../components/Page\"\nimport { graphql } from \"gatsby\"\nimport * as styles from \"../../style/agri.module.css\"\nimport { GatsbyImage } from \"gatsby-plugin-image\";\n\nfunction agri({ data }) {\n  var filteredData = {}\n\n  data.agri.edges.forEach(edge => {\n    filteredData[edge.node.name] = edge.node.childImageSharp.gatsbyImageData\n  })\n\n  return (\n    <Page title=\"Agri-Cane project\">\n      <div className={styles.container}>\n        <h2>AGRI-CANE</h2>\n        <h4 style={{ margin: \"0px 0px 50px\", textAlign: \"center\" }}>\n          Check it out on{\" \"}\n          <a\n            target=\"_blank\"\n            rel=\"noopener noreferrer\"\n            href=\"https://github.com/veeprayas/MAIN\"\n            style={{ color: \"blue\", textDecoration: \"underline\" }}\n          >\n            Github\n          </a>\n        </h4>\n        <div className={styles.float}>\n          <div>\n            <GatsbyImage image={filteredData.block} />\n            <p className={styles.figName}>Fig: Basic Block Diagram</p>\n          </div>\n          <div>\n            <div>\n              <h3>Introduction</h3>\n              <p>\n                In today’s era, farmers face a lot of problems while growing\n                their crops. This could be due to lack of insight on the growth\n                requirements of the crop or due to environmental factors. In our\n                project we highlight on the growth of sugarcane from sowing till\n                cutting and monitor every aspect throughout.\n              </p>\n              <h3>Scope</h3>\n              <p>\n                I assisted{\" \"}\n                <a\n                  href=\"https://www.linkedin.com/in/yash-prakash-030292162\"\n                  target=\"_blank\"\n                  rel=\"noopener noreferrer\"\n                  style={{ color: \"blue\" }}\n                >\n                  Yash Prakash\n                </a>{\" \"}\n                in specifically implementing the Image Classification model of\n                this project. Deep-Learning applied to computer vision was\n                implemented to monitor leaves of sugarcane and indicate whether\n                it is infected with yellow leaf syndrome or red dot disease,\n                which are the two prominent diseases which attack sugarcane, and\n                can be detected using image processing techniques. This would\n                help the farmers increase their yield.\n              </p>\n              <h3>My Objectives</h3>\n              <ul style={{ listStyleType: \"disc\" }}>\n                <li>\n                  To help the farmer by giving productive information about the\n                  health of his/her farm.\n                </li>\n                <li>\n                  Employ state of the art deep learning to achieve maximum\n                  accuracy.\n                </li>\n              </ul>\n            </div>\n          </div>\n        </div>\n        <h3>The Software</h3>\n        <p>\n          There are three main steps involved in the Image Processing portion of\n          this project.\n        </p>\n        <ul>\n          <li>\n            Pick an existing state-of-art Deep Learning Model for our image\n            classification.\n          </li>\n          <li>\n            Train it on an extensive image dataset such as Imagenet so it can\n            learn to extract the different features from images.\n          </li>\n          <li>\n            Apply some magic from Transfer Learning to make it \"transfer\" its\n            learnt skills of feature extraction on our sugarcanes, and detect\n            the presence of any disease.\n          </li>\n        </ul>\n        <h4>INCEPTION V3</h4>\n        <p>\n          Inception v3 is a widely-used image recognition model that has been\n          shown to attain greater than 78.1% accuracy on the ImageNet dataset.\n          The model is the culmination of many ideas developed by multiple\n          researchers over the years. It is based on the original\n          paper:\"Rethinking the Inception Architecture for Computer Vision\" by\n          Szegedy.\n          <br />\n          <br />\n          Inception V3 is an example of a Convolution Neural Network which has\n          two parts:\n        </p>\n        <ul style={{ listStyleType: \"disc\" }}>\n          <li>\n            A convolution tool that splits the various features of the image for\n            analysis\n          </li>\n          <li>\n            A fully connected layer that uses the output of the convolution\n            layer to predict the best description for the image.\n          </li>\n        </ul>\n        <br />\n        The model itself is made up of symmetric and asymmetric building blocks,\n        including convolutions, average pooling, max pooling dropouts, and fully\n        connected layers. Batch norm is used extensively throughout the model\n        and applied to activation inputs. Loss is computed via Softmax. A\n        high-level diagram of the model is shown below:{\" \"}\n        <div>\n          <GatsbyImage image={filteredData.inception} />\n          <p className={styles.figName}>Fig: Inception v3 model</p>\n        </div>\n        <p>\n          The different layers used in the Deep learning model and their\n          functions are highlighted below\n        </p>\n        <ul>\n          <li>\n            <p>\n              <b>Convolution Layer:</b>These layers employ different sets of\n              filters, typically hundreds-thousands and combines the results,\n              feeding the output into the next layer. This layer has \"filters\"\n              that automatically detects \"values\" for its filters and detects\n              objects in steps{\" \"}\n            </p>\n            <ul>\n              <li>Detect \"Edges\" from pixel intensities</li>\n              <li>use \"Edges\" to detect \"Shapes\"</li>\n              <li>\n                use \"Shapes\" to detect high-level features like facial\n                structures or parts of a leaf.\n              </li>\n            </ul>\n          </li>\n          <li>\n            <p>\n              <b>Activation Layer:</b>After each CONV layer in a CNN, we apply a\n              nonlinear activation function, such as ReLU, ELU etc. Activation\n              layers are not technically “layers” (due to the fact that no\n              parameters/weights are learned inside an activation layer) and are\n              sometimes omitted from network architecture diagrams as it’s\n              assumed that an activation immediately follows a convolution.\n            </p>\n          </li>\n          <li>\n            <p>\n              <b>Pooling Layer:</b>It is common to insert POOL layers in-between\n              consecutive convolution layers. The primary function of the POOL\n              layer is to progressively reduce the spatial size (i.e., width and\n              height) of the input volume. Doing this allows us to reduce the\n              amount of parameters and computation in the network – pooling also\n              helps us control overfitting. Max pooling is typically done in the\n              middle of the CNN architecture to reduce spatial size, whereas\n              average pooling is normally used as the final layer of the network\n              (e.x., GoogLeNet, SqueezeNet, ResNet) where we wish to avoid using\n              FC layers entirely.\n            </p>\n          </li>\n          <li>\n            <p>\n              <b>Fully-Connected Layer:</b>Neurons in FC layers are\n              fully-connected to all activations in the previous layer, as is\n              the standard for feedforward neural networks. FC layers are always\n              placed at the end of the network.\n            </p>\n          </li>\n          <li>\n            <p>\n              <b>Dropout Layer:</b>Dropout is actually a form of regularization\n              that aims to help prevent overfitting by increasing testing\n              accuracy, perhaps at the expense of training accuracy. For each\n              mini-batch in our training set, dropout layers, with probability\n              p, randomly disconnect inputs from the preceding layer to the next\n              layer in the network architecture. The reason we apply dropout is\n              to reduce overfitting by explicitly altering the network\n              architecture at training time. Randomly dropping connections\n              ensures that no single node in the network is responsible for\n              “activating” when presented with a given pattern. Instead, dropout\n              ensures there are multiple, redundant nodes that will activate\n              when presented with similar inputs – this in turn helps our model\n              to generalize.\n            </p>\n          </li>\n        </ul>\n        <p>\n          Before the model can be used to recognize images, it must be trained.\n          This is usually done via supervised learning using a large set of\n          labeled images. Although Inception v3 can be trained from many\n          different labeled image sets,&nbsp;ImageNet&nbsp;is a common dataset\n          of choice. ImageNet has over ten million URLs of labeled images. About\n          a million of the images also have bounding boxes specifying a more\n          precise location for the labeled objects. For this model, the ImageNet\n          dataset is composed of 1,331,167 images which are split into training\n          and evaluation datasets containing 1,281,167 and 50,000 images,\n          respectively. The training and evaluation datasets are kept separate\n          intentionally. Only images from the training dataset are used to train\n          the model and only images from the evaluation dataset are used to\n          evaluate model accuracy.\n        </p>\n        <h3>Transfer Learning</h3>\n        <div>\n          <GatsbyImage image={filteredData.transfer} />\n          <p className={styles.figName}>Fig: Transfer Learning</p>\n        </div>\n        <p>\n          Transfer learning is the improvement of learning in a new task through\n          the transfer of knowledge from a related task that has already been\n          learned.\n          <br />\n          Transfer learning is a machine learning method where a model developed\n          for a task is reused as the starting point for a model on a second\n          task.\n          <br />\n          It is a popular approach in deep learning where pre-trained models are\n          used as the starting point on computer vision and natural language\n          processing tasks given the vast compute and time resources required to\n          develop neural network models on these problems and from the huge\n          jumps in skill that they provide on related problems.\n        </p>\n        <p>\n          It is common to perform transfer learning with predictive modeling\n          problems that use image data as input.\n          <br /> This may be a prediction task that takes photographs or video\n          data as input.\n          <br /> For these types of problems, it is common to use a deep\n          learning model pre-trained for a large and challenging image\n          classification task such as the ImageNet 1000-class photograph\n          classification competition.\n          <br /> The research organizations that develop models for this\n          competition and do well often release their final model under a\n          permissive license for reuse. These models can take days or weeks to\n          train on modern hardware.\n          <br />\n          <br /> These models can be downloaded and incorporated directly into\n          new models that expect image data as input. <br />\n          <br />\n          This approach is effective because the images were trained on a large\n          corpus of photographs and require the model to make predictions on a\n          relatively large number of classes, in turn, requiring that the model\n          efficiently learn to extract features from photographs in order to\n          perform well on the problem.\n        </p>\n        <h3>Proposed Model</h3>\n        <div>\n          <GatsbyImage\n            image={filteredData.proposed}\n            style={{ maxWidth: \"500px\", margin: \"auto\" }} />\n          <p className={styles.figName}>Fig: Proposed Model</p>\n        </div>\n        <p>\n          <b>Step 1:</b> Take input as the image (crop-disease pair image).\n          <br />\n          <b>Step 2:</b>Pre-processing plant images.\n          <br />\n          <b>Step 3:</b> Train the model with leaf disease.\n          <br />\n          <b>Step 4:</b> CNN Validation stage where we can increase the\n          efficiency before make any test, which is sort of as the development\n          environment. <br />\n          <b>Step 5:</b> Test the model <br />\n          <b>Step 6:</b> A website will appear where user can identify whether\n          the leaf is diseased or healthy. The main aim is to design a system\n          which is efficient and which provide disease name. For that purpose we\n          use two phase: 1st is training phase and 2nd is testing phase.\n          <br />\n          <br />\n          <b>\n            In 1\n            <span style={{ verticalAlign: \"super\", fontSize: \"10px\" }}>st</span>{\" \"}\n            phase:\n          </b>{\" \"}\n          Image acquisition, Image Pre-processing and CNN based training. <br />\n          <b>\n            In 2\n            <span style={{ verticalAlign: \"super\", fontSize: \"10px\" }}>nd</span>{\" \"}\n            phase:\n          </b>{\" \"}\n          Image acquisition, Image Pre-processing, Classification and disease\n          identification and pesticides identification.\n          <br />\n          <br />\n          <b style={{ fontSize: \"16px\" }}>**NOTE**</b>Due to the{\" \"}\n          <b>COVID-19 outbreak</b>, we were unsuccessful in gathering sufficient\n          image data to train our classifier on yellow leaf syndrome or red dot\n          disease as most travel was prohibited. Therefore, for experimentation\n          purpose we have used <b>Plant Village datasets</b>. The data records\n          contain 54,000 images. The images span 14 crop species: Apple,\n          Blueberry, Cherry, Corn, Grape, Orange, Peach, Bell Pepper, Potato,\n          Raspberry, Soybean, Squash, Strawberry, and Tomato. It contains images\n          of{\" \"}\n          <b>\n            17 fungal diseases, 4 bacterial diseases, 2 mold (oomycete)\n            diseases, 2 viral disease, and 1 disease caused by a mite\n          </b>\n          . 12 crop species also have images of healthy leaves that are{\" \"}\n          <b>not visibly </b>affected by a disease.\n        </p>\n        <div>\n          <GatsbyImage\n            image={filteredData.snippet}\n            style={{ maxWidth: \"500px\", margin: \"auto\" }} />\n          <p className={styles.figName}>Fig: Snippet of Dataset</p>\n        </div>\n        <h3>RESULTS</h3>\n        <p>\n          Here is the final webapp landing page where the farmer is expected to\n          upload the image of the suagarcane leaf to check if it is diseased or\n          not.(Developed by Yash)\n          <br />\n          The uploaded image is sent to the back end where the image processing\n          techniques are used to determine the state of the crop.\n        </p>\n        <div>\n          <GatsbyImage image={filteredData.landing} />\n          <p className={styles.figName}>\n            Fig: Web Application Image Upload page\n          </p>\n        </div>\n        <p>\n          After training the Inception v3 model on the ImageNet Dataset and\n          applying transfer learning to classify our required dataset, we end up\n          with the following Model Classification Report.\n        </p>\n        <div>\n          <GatsbyImage image={filteredData.Report} style={{ maxWidth: \"500px\", margin: \"auto\" }} />\n          <p className={styles.figName}>Fig: Model Classification Report</p>\n        </div>\n        <p>\n          {\" \"}\n          The matrix below shows which class of crop/ disease the picture taken\n          resembles and has highest probability with respect to the picture\n          uploaded. In this case the position 1 is of highest probability and\n          hence resembles class one disease. The matrix position with the\n          highest probability represents the class of leaf (image uploaded by\n          the farmer). Hence, this is used to say whether that particular crop\n          is diseased or not.\n        </p>\n        <div className={styles.float}>\n          <div>\n            <GatsbyImage image={filteredData.table} />\n            <p className={styles.figName}>\n              Fig: classification table for probability matrix verification\n            </p>\n          </div>\n\n          <div>\n            <GatsbyImage image={filteredData.matrix} />\n            <p className={styles.figName}>\n              Fig: Probability Matrix obtained after image processing\n            </p>\n          </div>\n        </div>\n      </div>\n    </Page>\n  );\n}\n\nexport const query = graphql`{\n  agri: allFile(filter: {relativeDirectory: {eq: \"agri\"}}) {\n    edges {\n      node {\n        name\n        childImageSharp {\n          gatsbyImageData(layout: FULL_WIDTH)\n        }\n      }\n    }\n  }\n}\n`\n\nexport default agri\n"],"sourceRoot":""}