(self.webpackChunkshoaib_alyaan_portfolio=self.webpackChunkshoaib_alyaan_portfolio||[]).push([[835],{4972:function(e,t,a){"use strict";a.d(t,{Z:function(){return r}});var n=a(7294);var l=a(7603),i=a(5444);var r=function(){return n.createElement("div",{className:"gohome-module--container--s_Y8P"},n.createElement("div",{className:"gohome-module--NavBar--20YcT"},n.createElement(l.Z,null),n.createElement("h2",{className:"gohome-module--home--3y76k"},n.createElement(i.Link,{to:"/"},n.createElement("svg",{className:"gohome-module--SHO--2h-T0",viewBox:"0 0 258 207",fill:"none",xmlns:"http://www.w3.org/2000/svg"},n.createElement("path",{d:"M246 154H155.5H12C7.02944 154 3 149.971 3 145V11.5C3 6.52944 7.02943 2.5 12 2.5H246C250.971 2.5 255 6.52943 255 11.5V145C255 149.971 250.971 154 246 154Z",fill:"#E94F37",stroke:"black",strokeWidth:"5"}),n.createElement("path",{d:"M104 155.5V204H154.5V155.5H104Z",fill:"none"}),n.createElement("path",{d:"M68.5001 204H187M113.5 122.5L128.5 81L143.5 39.5M91.5001 110L31.0001 78.5L91.5001 47M168.5 110L224.5 78.5L168.5 47M104 204V155.5H154.5V204H104Z",stroke:"black",strokeWidth:"5"}),n.createElement("path",{d:"M99.205 106.65C96.475 106.65 94.3517 106.26 92.835 105.48C91.3617 104.7 90.625 103.812 90.625 102.815C90.625 102.252 90.7333 101.623 90.95 100.93C91.21 100.237 91.5567 99.6083 91.99 99.045C93.68 100.562 95.9117 101.32 98.685 101.32C102.975 101.32 105.12 99.11 105.12 94.69C105.12 93.13 104.838 91.7433 104.275 90.53C103.712 89.2733 102.715 87.8 101.285 86.11L94.785 78.505C93.3117 76.7283 92.25 75.125 91.6 73.695C90.9933 72.2217 90.69 70.705 90.69 69.145C90.69 66.3283 91.665 64.075 93.615 62.385C95.6083 60.695 98.36 59.85 101.87 59.85C104.773 59.85 106.918 60.1533 108.305 60.76C109.692 61.3233 110.385 62.1033 110.385 63.1C110.385 64.1833 109.952 65.18 109.085 66.09C108.608 65.7 107.763 65.3533 106.55 65.05C105.38 64.7033 104.102 64.53 102.715 64.53C100.765 64.53 99.205 64.92 98.035 65.7C96.9083 66.48 96.345 67.585 96.345 69.015C96.345 70.1417 96.5833 71.1817 97.06 72.135C97.5367 73.045 98.295 74.15 99.335 75.45L106.42 83.445C108.067 85.395 109.193 87.1933 109.8 88.84C110.45 90.4867 110.775 92.35 110.775 94.43C110.775 98.1133 109.865 101.082 108.045 103.335C106.268 105.545 103.322 106.65 99.205 106.65ZM136.021 104.96C136.021 105.35 135.934 105.61 135.761 105.74C135.631 105.87 135.133 105.957 134.266 106C133.443 106 132.294 106 130.821 106V83.965H120.421V104.96C120.421 105.35 120.334 105.61 120.161 105.74C120.031 105.87 119.533 105.957 118.666 106C117.799 106 116.629 106 115.156 106V61.54C115.156 61.15 115.221 60.89 115.351 60.76C115.524 60.63 116.044 60.565 116.911 60.565C117.778 60.5217 118.948 60.5 120.421 60.5V79.025H130.821V61.54C130.821 61.15 130.886 60.89 131.016 60.76C131.189 60.63 131.688 60.565 132.511 60.565C133.378 60.5217 134.548 60.5 136.021 60.5V104.96ZM152.671 106.65C149.724 106.65 147.406 105.978 145.716 104.635C144.069 103.248 142.856 100.865 142.076 97.485C141.296 94.0617 140.906 89.23 140.906 82.99C140.906 77.27 141.339 72.72 142.206 69.34C143.116 65.96 144.481 63.5333 146.301 62.06C148.121 60.5867 150.483 59.85 153.386 59.85C156.246 59.85 158.499 60.565 160.146 61.995C161.793 63.3817 162.984 65.7217 163.721 69.015C164.501 72.3083 164.891 76.9017 164.891 82.795C164.891 88.7317 164.458 93.4333 163.591 96.9C162.768 100.367 161.468 102.858 159.691 104.375C157.914 105.892 155.574 106.65 152.671 106.65ZM152.866 101.255C154.469 101.255 155.726 100.648 156.636 99.435C157.546 98.2217 158.196 96.2933 158.586 93.65C159.019 91.0067 159.236 87.4317 159.236 82.925C159.236 78.4183 159.063 74.9083 158.716 72.395C158.369 69.8817 157.763 68.0617 156.896 66.935C156.029 65.8083 154.816 65.245 153.256 65.245C151.696 65.245 150.439 65.8733 149.486 67.13C148.576 68.3433 147.904 70.3367 147.471 73.11C147.038 75.84 146.821 79.5667 146.821 84.29C146.821 88.45 146.994 91.765 147.341 94.235C147.731 96.6617 148.359 98.4383 149.226 99.565C150.093 100.692 151.306 101.255 152.866 101.255Z",fill:"white"}))))))}},7603:function(e,t,a){"use strict";a.d(t,{Z:function(){return o}});var n=a(7294),l=a(96);const i="nav-module--bar--1Fbv1",r="nav-module--item--2VeQd";var o=function(){var e=(0,n.useState)(!1),t=e[0],a=e[1];return(0,n.useEffect)((function(){t?(Object.assign(document.body.style,{height:"100vh",overflow:"hidden"}),setTimeout((function(){var e=document.getElementById("NavContainer");Object.assign(e.style,{display:"flex"}),(0,l.tS)({elements:["#Home","#Internship","#Aproject"],style:{transform:"translate(0px)",opacity:"1"},delay:100})}),1e3)):(Object.assign(document.body.style,{height:"100%",overflow:"scroll"}),(0,l.tS)({elements:["#Home","#Internship","#Aproject"],style:{transform:"translate(50px)",opacity:"0"},delay:100}),setTimeout((function(){var e=document.getElementById("NavContainer");Object.assign(e.style,{display:"none"})}),1e3))}),[t]),n.createElement("div",{className:"nav-module--container--1uXNs"},n.createElement("div",{onClick:function(){a((function(e){return!e}))},className:t?"nav-module--closeIcon--3m9w7":"nav-module--openIcon--kco8V"},n.createElement("div",{className:i+" nav-module--bar1--2cQv9"}),n.createElement("div",{className:i+" nav-module--bar2--3G4C7"}),n.createElement("div",{className:i+" nav-module--bar3--1_HUK"})),n.createElement("nav",{id:"NavContainer",className:"nav-module--itemContainer--8kuFG"},n.createElement("ul",{className:t?"nav-module--itemsVisible--3EL9m":"nav-module--itemsInvisible--1satG"},n.createElement("li",{className:"nav-module--navTitle--dG0n4"},"SHOAIB ALYAAN"),n.createElement("li",{id:"Home",className:r},n.createElement("a",{"aria-current":"page",className:"",href:"/"},n.createElement("h2",null,"Home"))),n.createElement("li",{id:"Internship",className:r},n.createElement("a",{href:"/internships"},n.createElement("h2",null,"Internships"))),n.createElement("li",{id:"Aproject",className:r},n.createElement("a",{href:"/projects"},n.createElement("h2",null,"Academic Projects"))))),n.createElement("div",{className:t?"nav-module--openBG--1_EfO":"nav-module--closeBG--2r85t"}))}},635:function(e,t,a){"use strict";var n=a(7294),l=a(3751),i=a(4972);t.Z=function(e){var t=e.children,a=e.title;return n.createElement("div",null,n.createElement(l.Z,{title:a}),n.createElement(i.Z,null),t)}},96:function(e,t,a){"use strict";function n(e){e&&e.parent||e&&e.elements&&("number"==typeof e.delay?e.elements.map((function(t,a){setTimeout((function(){l(t,e.style)}),e.delay+a*e.delay)})):e.elements.map((function(t,a){setTimeout((function(){l(t,e.style)}),e.delay[a]+a*e.delay[a])})))}a.d(t,{tS:function(){return n},zJ:function(){return i}});var l=function(e,t){Object.assign(document.querySelector(e).style,t)},i=function(e){if(e.cname)if(1==e.children){var t,a=document.querySelector("."+e.cname);t=Array.prototype.slice.call(document.querySelector("."+e.cname).childNodes),e.style?r(a)&&t.map((function(t,a){setTimeout((function(){Object.assign(t.style,e.style)}),e.delay+a*e.delay)})):e.styleClass&&r(a)&&t.map((function(t,a){setTimeout((function(){t.classList.add(e.styleClass)}),e.delay+a*e.delay)}))}else t=Array.prototype.slice.call(document.querySelectorAll("."+e.cname)),e.style?t.forEach((function(t){r(t)?Object.assign(t.style,e.style):Object.assign(t.style,e.rstyle)})):e.styleClass&&t.forEach((function(t){r(t)?t.classList.add(e.styleClass):t.classList.remove(e.styleClass)}))},r=function(e,t,a){void 0===t&&(t=.2),void 0===a&&(a=.8);var n=e.getBoundingClientRect();return n.top>window.innerHeight*t&&n.top<window.innerHeight*a}},2602:function(e,t,a){"use strict";a.r(t),a.d(t,{default:function(){return s}});var n=a(7294),l=a(635);const i="agri-module--float--16NsZ",r="agri-module--figName---liQI";var o=a(154);var s=function(e){var t=e.data,a={};return t.agri.edges.forEach((function(e){a[e.node.name]=e.node.childImageSharp.gatsbyImageData})),n.createElement(l.Z,{title:"Agri-Cane project"},n.createElement("div",{className:"agri-module--container--3Nrft"},n.createElement("h2",null,"AGRI-CANE"),n.createElement("h4",{style:{margin:"0px 0px 50px",textAlign:"center"}},"Check it out on"," ",n.createElement("a",{target:"_blank",rel:"noopener noreferrer",href:"https://github.com/veeprayas/MAIN",style:{color:"blue",textDecoration:"underline"}},"Github")),n.createElement("div",{className:i},n.createElement("div",null,n.createElement(o.G,{image:a.block}),n.createElement("p",{className:r},"Fig: Basic Block Diagram")),n.createElement("div",null,n.createElement("div",null,n.createElement("h3",null,"Introduction"),n.createElement("p",null,"In today’s era, farmers face a lot of problems while growing their crops. This could be due to lack of insight on the growth requirements of the crop or due to environmental factors. In our project we highlight on the growth of sugarcane from sowing till cutting and monitor every aspect throughout."),n.createElement("h3",null,"Scope"),n.createElement("p",null,"I assisted"," ",n.createElement("a",{href:"https://www.linkedin.com/in/yash-prakash-030292162",target:"_blank",rel:"noopener noreferrer",style:{color:"blue"}},"Yash Prakash")," ","in specifically implementing the Image Classification model of this project. Deep-Learning applied to computer vision was implemented to monitor leaves of sugarcane and indicate whether it is infected with yellow leaf syndrome or red dot disease, which are the two prominent diseases which attack sugarcane, and can be detected using image processing techniques. This would help the farmers increase their yield."),n.createElement("h3",null,"My Objectives"),n.createElement("ul",{style:{listStyleType:"disc"}},n.createElement("li",null,"To help the farmer by giving productive information about the health of his/her farm."),n.createElement("li",null,"Employ state of the art deep learning to achieve maximum accuracy."))))),n.createElement("h3",null,"The Software"),n.createElement("p",null,"There are three main steps involved in the Image Processing portion of this project."),n.createElement("ul",null,n.createElement("li",null,"Pick an existing state-of-art Deep Learning Model for our image classification."),n.createElement("li",null,"Train it on an extensive image dataset such as Imagenet so it can learn to extract the different features from images."),n.createElement("li",null,'Apply some magic from Transfer Learning to make it "transfer" its learnt skills of feature extraction on our sugarcanes, and detect the presence of any disease.')),n.createElement("h4",null,"INCEPTION V3"),n.createElement("p",null,'Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years. It is based on the original paper:"Rethinking the Inception Architecture for Computer Vision" by Szegedy.',n.createElement("br",null),n.createElement("br",null),"Inception V3 is an example of a Convolution Neural Network which has two parts:"),n.createElement("ul",{style:{listStyleType:"disc"}},n.createElement("li",null,"A convolution tool that splits the various features of the image for analysis"),n.createElement("li",null,"A fully connected layer that uses the output of the convolution layer to predict the best description for the image.")),n.createElement("br",null),"The model itself is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling dropouts, and fully connected layers. Batch norm is used extensively throughout the model and applied to activation inputs. Loss is computed via Softmax. A high-level diagram of the model is shown below:"," ",n.createElement("div",null,n.createElement(o.G,{image:a.inception}),n.createElement("p",{className:r},"Fig: Inception v3 model")),n.createElement("p",null,"The different layers used in the Deep learning model and their functions are highlighted below"),n.createElement("ul",null,n.createElement("li",null,n.createElement("p",null,n.createElement("b",null,"Convolution Layer:"),'These layers employ different sets of filters, typically hundreds-thousands and combines the results, feeding the output into the next layer. This layer has "filters" that automatically detects "values" for its filters and detects objects in steps'," "),n.createElement("ul",null,n.createElement("li",null,'Detect "Edges" from pixel intensities'),n.createElement("li",null,'use "Edges" to detect "Shapes"'),n.createElement("li",null,'use "Shapes" to detect high-level features like facial structures or parts of a leaf.'))),n.createElement("li",null,n.createElement("p",null,n.createElement("b",null,"Activation Layer:"),"After each CONV layer in a CNN, we apply a nonlinear activation function, such as ReLU, ELU etc. Activation layers are not technically “layers” (due to the fact that no parameters/weights are learned inside an activation layer) and are sometimes omitted from network architecture diagrams as it’s assumed that an activation immediately follows a convolution.")),n.createElement("li",null,n.createElement("p",null,n.createElement("b",null,"Pooling Layer:"),"It is common to insert POOL layers in-between consecutive convolution layers. The primary function of the POOL layer is to progressively reduce the spatial size (i.e., width and height) of the input volume. Doing this allows us to reduce the amount of parameters and computation in the network – pooling also helps us control overfitting. Max pooling is typically done in the middle of the CNN architecture to reduce spatial size, whereas average pooling is normally used as the final layer of the network (e.x., GoogLeNet, SqueezeNet, ResNet) where we wish to avoid using FC layers entirely.")),n.createElement("li",null,n.createElement("p",null,n.createElement("b",null,"Fully-Connected Layer:"),"Neurons in FC layers are fully-connected to all activations in the previous layer, as is the standard for feedforward neural networks. FC layers are always placed at the end of the network.")),n.createElement("li",null,n.createElement("p",null,n.createElement("b",null,"Dropout Layer:"),"Dropout is actually a form of regularization that aims to help prevent overfitting by increasing testing accuracy, perhaps at the expense of training accuracy. For each mini-batch in our training set, dropout layers, with probability p, randomly disconnect inputs from the preceding layer to the next layer in the network architecture. The reason we apply dropout is to reduce overfitting by explicitly altering the network architecture at training time. Randomly dropping connections ensures that no single node in the network is responsible for “activating” when presented with a given pattern. Instead, dropout ensures there are multiple, redundant nodes that will activate when presented with similar inputs – this in turn helps our model to generalize."))),n.createElement("p",null,"Before the model can be used to recognize images, it must be trained. This is usually done via supervised learning using a large set of labeled images. Although Inception v3 can be trained from many different labeled image sets, ImageNet is a common dataset of choice. ImageNet has over ten million URLs of labeled images. About a million of the images also have bounding boxes specifying a more precise location for the labeled objects. For this model, the ImageNet dataset is composed of 1,331,167 images which are split into training and evaluation datasets containing 1,281,167 and 50,000 images, respectively. The training and evaluation datasets are kept separate intentionally. Only images from the training dataset are used to train the model and only images from the evaluation dataset are used to evaluate model accuracy."),n.createElement("h3",null,"Transfer Learning"),n.createElement("div",null,n.createElement(o.G,{image:a.transfer}),n.createElement("p",{className:r},"Fig: Transfer Learning")),n.createElement("p",null,"Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned.",n.createElement("br",null),"Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.",n.createElement("br",null),"It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems."),n.createElement("p",null,"It is common to perform transfer learning with predictive modeling problems that use image data as input.",n.createElement("br",null)," This may be a prediction task that takes photographs or video data as input.",n.createElement("br",null)," For these types of problems, it is common to use a deep learning model pre-trained for a large and challenging image classification task such as the ImageNet 1000-class photograph classification competition.",n.createElement("br",null)," The research organizations that develop models for this competition and do well often release their final model under a permissive license for reuse. These models can take days or weeks to train on modern hardware.",n.createElement("br",null),n.createElement("br",null)," These models can be downloaded and incorporated directly into new models that expect image data as input. ",n.createElement("br",null),n.createElement("br",null),"This approach is effective because the images were trained on a large corpus of photographs and require the model to make predictions on a relatively large number of classes, in turn, requiring that the model efficiently learn to extract features from photographs in order to perform well on the problem."),n.createElement("h3",null,"Proposed Model"),n.createElement("div",null,n.createElement(o.G,{image:a.proposed,style:{maxWidth:"500px",margin:"auto"}}),n.createElement("p",{className:r},"Fig: Proposed Model")),n.createElement("p",null,n.createElement("b",null,"Step 1:")," Take input as the image (crop-disease pair image).",n.createElement("br",null),n.createElement("b",null,"Step 2:"),"Pre-processing plant images.",n.createElement("br",null),n.createElement("b",null,"Step 3:")," Train the model with leaf disease.",n.createElement("br",null),n.createElement("b",null,"Step 4:")," CNN Validation stage where we can increase the efficiency before make any test, which is sort of as the development environment. ",n.createElement("br",null),n.createElement("b",null,"Step 5:")," Test the model ",n.createElement("br",null),n.createElement("b",null,"Step 6:")," A website will appear where user can identify whether the leaf is diseased or healthy. The main aim is to design a system which is efficient and which provide disease name. For that purpose we use two phase: 1st is training phase and 2nd is testing phase.",n.createElement("br",null),n.createElement("br",null),n.createElement("b",null,"In 1",n.createElement("span",{style:{verticalAlign:"super",fontSize:"10px"}},"st")," ","phase:")," ","Image acquisition, Image Pre-processing and CNN based training. ",n.createElement("br",null),n.createElement("b",null,"In 2",n.createElement("span",{style:{verticalAlign:"super",fontSize:"10px"}},"nd")," ","phase:")," ","Image acquisition, Image Pre-processing, Classification and disease identification and pesticides identification.",n.createElement("br",null),n.createElement("br",null),n.createElement("b",{style:{fontSize:"16px"}},"**NOTE**"),"Due to the"," ",n.createElement("b",null,"COVID-19 outbreak"),", we were unsuccessful in gathering sufficient image data to train our classifier on yellow leaf syndrome or red dot disease as most travel was prohibited. Therefore, for experimentation purpose we have used ",n.createElement("b",null,"Plant Village datasets"),". The data records contain 54,000 images. The images span 14 crop species: Apple, Blueberry, Cherry, Corn, Grape, Orange, Peach, Bell Pepper, Potato, Raspberry, Soybean, Squash, Strawberry, and Tomato. It contains images of"," ",n.createElement("b",null,"17 fungal diseases, 4 bacterial diseases, 2 mold (oomycete) diseases, 2 viral disease, and 1 disease caused by a mite"),". 12 crop species also have images of healthy leaves that are"," ",n.createElement("b",null,"not visibly "),"affected by a disease."),n.createElement("div",null,n.createElement(o.G,{image:a.snippet,style:{maxWidth:"500px",margin:"auto"}}),n.createElement("p",{className:r},"Fig: Snippet of Dataset")),n.createElement("h3",null,"RESULTS"),n.createElement("p",null,"Here is the final webapp landing page where the farmer is expected to upload the image of the suagarcane leaf to check if it is diseased or not.(Developed by Yash)",n.createElement("br",null),"The uploaded image is sent to the back end where the image processing techniques are used to determine the state of the crop."),n.createElement("div",null,n.createElement(o.G,{image:a.landing}),n.createElement("p",{className:r},"Fig: Web Application Image Upload page")),n.createElement("p",null,"After training the Inception v3 model on the ImageNet Dataset and applying transfer learning to classify our required dataset, we end up with the following Model Classification Report."),n.createElement("div",null,n.createElement(o.G,{image:a.Report,style:{maxWidth:"500px",margin:"auto"}}),n.createElement("p",{className:r},"Fig: Model Classification Report")),n.createElement("p",null," ","The matrix below shows which class of crop/ disease the picture taken resembles and has highest probability with respect to the picture uploaded. In this case the position 1 is of highest probability and hence resembles class one disease. The matrix position with the highest probability represents the class of leaf (image uploaded by the farmer). Hence, this is used to say whether that particular crop is diseased or not."),n.createElement("div",{className:i},n.createElement("div",null,n.createElement(o.G,{image:a.table}),n.createElement("p",{className:r},"Fig: classification table for probability matrix verification")),n.createElement("div",null,n.createElement(o.G,{image:a.matrix}),n.createElement("p",{className:r},"Fig: Probability Matrix obtained after image processing")))))}}}]);
//# sourceMappingURL=component---src-pages-projects-agri-js-cd7ebe29d47db9d04efa.js.map